{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd315ae",
   "metadata": {},
   "source": [
    "## CHATOLLAMA\n",
    "\n",
    "Notebook creato per testare l'utilizzo \"base\" di ChatOllama sulla base della documentazione ufficiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0969401",
   "metadata": {},
   "source": [
    "Visualizzo i modelli disponibili sull'istanza di Ollama \"locale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd925e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                         ID              SIZE      MODIFIED    \r\n",
      "llama3.2:3b-instruct-fp16    195a8c01d91e    6.4 GB    12 days ago    \r\n",
      "llama3.2:latest              a80c4f17acd5    2.0 GB    12 days ago    \r\n",
      "llama3.1:latest              42182419e950    4.7 GB    3 weeks ago    \r\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac8213",
   "metadata": {},
   "source": [
    "Per avere un maggior dettaglio riguardo i modelli presenti su Ollama posso utilizzare la libreria Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b50f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'llama3.2:3b-instruct-fp16',\n",
       "   'model': 'llama3.2:3b-instruct-fp16',\n",
       "   'modified_at': '2024-09-29T22:07:48.412703619+02:00',\n",
       "   'size': 6433703586,\n",
       "   'digest': '195a8c01d91ec3cb1e0aad4624a51f2602c51fa7d96110f8ab5a20c84081804d',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '3.2B',\n",
       "    'quantization_level': 'F16'}},\n",
       "  {'name': 'llama3.2:latest',\n",
       "   'model': 'llama3.2:latest',\n",
       "   'modified_at': '2024-09-29T17:42:43.594752215+02:00',\n",
       "   'size': 2019393189,\n",
       "   'digest': 'a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '3.2B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'llama3.1:latest',\n",
       "   'model': 'llama3.1:latest',\n",
       "   'modified_at': '2024-09-14T19:18:52.515326323+02:00',\n",
       "   'size': 4661230766,\n",
       "   'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31766949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd08a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je aime le programmation.\\n\\n(Note: I used the informal \"je\" instead of the formal \"on\" because it\\'s more suitable for casual conversations.)', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-12T10:50:15.004376373Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4917180401, 'load_duration': 1809238691, 'prompt_eval_count': 45, 'prompt_eval_duration': 816539000, 'eval_count': 32, 'eval_duration': 2203406000}, id='run-5e2f171c-6a63-407c-9072-3c2f19340390-0', usage_metadata={'input_tokens': 45, 'output_tokens': 32, 'total_tokens': 77})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages, config={'callbaks' : [ConsoleCallbackHandler]})\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a54ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je aime le programmation.\n",
      "\n",
      "(Note: I used the informal \"je\" instead of the formal \"on\" because it's more suitable for casual conversations.)\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4420b",
   "metadata": {},
   "source": [
    "Langchain, attraverso l'utilizzo di LCEL consente di concatenare, in una chain, il modello creato con ChatOllama con un PrompTemplate al fine di avere una maggiore possibilità di customizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143fa74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ti piace molto programmare con Python.\\n\\n(Translation note: I\\'ve used the phrase \"ti piace molto\" which is a more informal way of saying \"I really like\" or \"I\\'m very fond of\". If you\\'d prefer a more formal translation, I can use \"ti piace\" instead.)\\n\\nVuoi parlare di un progetto specifico che stai lavorando con Python?', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-12T10:50:53.423933769Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 6377153542, 'load_duration': 18560456, 'prompt_eval_count': 42, 'prompt_eval_duration': 325451000, 'eval_count': 83, 'eval_duration': 5904416000}, id='run-ce1ff594-db51-4b26-ab25-71791bb939e7-0', usage_metadata={'input_tokens': 42, 'output_tokens': 83, 'total_tokens': 125})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Italian\",\n",
    "        \"input\": \"I love programming in python.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf980e",
   "metadata": {},
   "source": [
    "PSe si utilizza un modello per il quale è stato fatto il fine-tune per renderlo compatibile con l'utilizzo dei tools è possibile richiamare funzioni dal LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15755b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'validate_user',\n",
       "  'args': {'addresses': '[\"123 Fake St\", \"234 Pretend Boulevard\"]',\n",
       "   'user_id': '123'},\n",
       "  'id': '9b7160f1-b280-4d44-b457-5ec22f2d65e0',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_user(user_id: int, addresses: List[str]) -> bool:\n",
    "    \"\"\"Validate user using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "        addresses (List[str]): Previous addresses as a list of strings.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ").bind_tools([validate_user])\n",
    "\n",
    "result = llm.invoke(\n",
    "    \"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX.\"\n",
    ")\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db9203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_env]",
   "language": "python",
   "name": "conda-env-langchain_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
