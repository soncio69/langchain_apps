{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be58b823-732f-4826-8c65-bd1c14ce317a",
   "metadata": {},
   "source": [
    "# RAG CON DEEPSEEK\n",
    "\n",
    "Obiettivo di questo notebook è testare l'utilizzo del nuovo modello Deepseek-r1 in una pipeline di RAG.<br>\n",
    "Per utilizzare localmente questo modello (in versione quantizzata 7B) utilizzo il tool Ollama.\n",
    "\n",
    "TODO : da rivedere il modello utilizzato per embeddings (in italiano)\n",
    "TODO : da approfondire l'utilizzo di Chroma (al momento se si esegue più volte aggiunge sempre documenti...)\n",
    "TODO : L'estrazione dei documenti non sembra sempre congruente con la domanda in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a77078-9bd3-4ecb-b201-795448576245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   ID              SIZE      MODIFIED     \n",
      "erwan2/DeepSeek-Janus-Pro-7B:latest    253d552064e2    4.2 GB    2 hours ago     \n",
      "MFDoom/deepseek-r1-tool-calling:7b     2410129d448f    4.7 GB    2 days ago      \n",
      "deepseek-r1:latest                     0a8c26691023    4.7 GB    3 days ago      \n",
      "qwen2.5-coder:3b                       e7149271c296    1.9 GB    2 weeks ago     \n",
      "sqlcoder:latest                        77ac14348387    4.1 GB    2 months ago    \n",
      "llama3.2-vision:latest                 38107a0cd119    7.9 GB    2 months ago    \n",
      "llama3.2:3b-instruct-fp16              195a8c01d91e    6.4 GB    4 months ago    \n",
      "llama3.2:latest                        a80c4f17acd5    2.0 GB    4 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80681446-e204-46a6-8935-a19057baba7e",
   "metadata": {},
   "source": [
    "Per l'implementazione utilizzo:\n",
    "- la libreria ollama\n",
    "- langchain\n",
    "- Chromadb : come vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69516138-9e97-4839-8b5c-be0e70805fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gradio as gr\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af4b45-856c-458c-807e-0e3c6fc7e997",
   "metadata": {},
   "source": [
    "Obiettivo del test è implementare una pipeline che, partendo da un pdf fornito in input, consenta l'interrogazione del pdf stesso al fine di rispondere a domande in linguaggio naturale riguardo il contenuto.\n",
    "\n",
    "Il pdf è statico (nel codice) ma è possibile conventire le funzione in un'applicazione web per gestire l'upload del pdf di input.<br>\n",
    "A tal fine si possono utilizzare librerie come Gradio o Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819273d-e50e-41c0-8090-34c733c37b89",
   "metadata": {},
   "source": [
    "Innanzi tutto creo una funzione di gestire il pdf in input.<br>\n",
    "La funzione riceve in input un pdf e predispone tutto il necessario per information retrieval.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1ff2bc-6758-4f30-a9a3-30bef62f430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_bytes):\n",
    "    if pdf_bytes is None:\n",
    "        print(\"NO PDF\")\n",
    "        return None, None, None\n",
    "    loader = PyMuPDFLoader(pdf_bytes)\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    print(f\"Ottenuti : {len(chunks)} documenti\")\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-r1\")\n",
    "    vectorstore=Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return text_splitter, vectorstore, retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec3f38-d90e-4cd2-adca-6d625341daf3",
   "metadata": {},
   "source": [
    "Creo un'altra funzione che unisce i vari documenti estratti in un'unica stringa che verrà passata come contesto al modello per produrre la risposta finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45cac3da-3418-4906-8300-96c570281aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd095d6f-1b03-4605-a992-980655ae6ce8",
   "metadata": {},
   "source": [
    "Questa funzione costituisce il cuore della pipeline.<br>\n",
    "Riceve in input:\n",
    "- la domanda posta dall'utente\n",
    "- il contesto costituito dalla concatenazione dei documenti estratti dal vectore store sulla base delle similarity search con la domanda fornita\n",
    "\n",
    "Utilizza queste 2 stringhe per creare il prompt da passare al modello deepseek-r1 in esecuzione su Ollama.\n",
    "A seguito della risposta del modello, si procede ad eliminare i \"thinking script\" presenti nelle risposte di deepseek-r1 per ottenere la risposta finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575e9844-1241-464d-869a-3a97c585a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ollama_llm(question, context):\n",
    "\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    print(f\"Richiamo il modello con {formatted_prompt}\")\n",
    "    response = ollama.chat(model=\"deepseek-r1\", messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    response_content = response['message']['content']\n",
    "    \n",
    "    # Remove content between <think> and </think> tags to remove thinking output\n",
    "    final_answer = re.sub(r'<think>.*?</think>', '', response_content, flags=re.DOTALL).strip()\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57196ff1-93db-4085-81de-468ff450b53d",
   "metadata": {},
   "source": [
    "Funzione di estrazione dei documenti dal vectore store sulla base di limilarity search con la domanda in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9eef2d8-c9be-4947-900c-e9c568ca62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(question, text_splitter, vectorstore, retriever):\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    print(f\"Estratti {len(retrieved_docs)} documenti\")\n",
    "    formatted_content = combine_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4c359-5f22-432e-b752-a9ca69f90518",
   "metadata": {},
   "source": [
    "Funzione principale che si occupa di gestire l'intera pipeline:\n",
    "1) riceve il pdf di input e lo predispone per information retrieval\n",
    "2) esegue la rag chain per ottenere la risposta finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ee1105-9fed-47b2-bd0c-41315c5468dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(pdf_bytes, question):\n",
    "    \n",
    "    print(\"Processo il pdf in input\")\n",
    "    text_splitter, vectorstore, retriever = process_pdf(pdf_bytes)\n",
    "    print(\"Pdf processato!\")\n",
    "    if text_splitter is None:\n",
    "        print(\"nessun pdf caricato\")\n",
    "        return None  # No PDF uploaded\n",
    "    print(\"Eseguo RAG Chain\")\n",
    "    result = rag_chain(question, text_splitter, vectorstore, retriever)\n",
    "    return {result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ec43d3-aae9-4d64-b0f7-8f5183f91baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf = \"Preliminare.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04336477-6040-4fa6-94ad-4c25bca70129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_question(pdf, \"Come si chiama il notaio che ha redatto l'atto?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99286bb9-2873-4f55-83bf-660ab4e461dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo il pdf in input\n",
      "Ottenuti : 15 documenti\n",
      "Pdf processato!\n",
      "Eseguo RAG Chain\n",
      "Estratti 4 documenti\n",
      "Richiamo il modello con Question: Qual'è il giorno di ingresso al museo?\n",
      "\n",
      "Context: Grazie per aver acquistato il biglietto di ingresso per la visita al nostro Museo.\n",
      "Non è necessario stampare il presente biglietto, è sufficiente mostrarlo all’ingresso del Museo in formato \n",
      "elettronico.\n",
      "Il biglietto ha validità nella data scelta in fase di acquisto e su di esso riportata.\n",
      "L'ultimo accesso al Museo è consentito 45 minuti prima dell'orario di chiusura.\n",
      "La visita ha la durata di un’ora circa.\n",
      "\n",
      "Grazie per aver acquistato il biglietto di ingresso per la visita al nostro Museo.\n",
      "Non è necessario stampare il presente biglietto, è sufficiente mostrarlo all’ingresso del Museo in formato \n",
      "elettronico.\n",
      "Il biglietto ha validità nella data scelta in fase di acquisto e su di esso riportata.\n",
      "L'ultimo accesso al Museo è consentito 45 minuti prima dell'orario di chiusura.\n",
      "La visita ha la durata di un’ora circa.\n",
      "\n",
      "Grazie per aver acquistato il biglietto di ingresso per la visita al nostro Museo.\n",
      "Non è necessario stampare il presente biglietto, è sufficiente mostrarlo all’ingresso del Museo in formato \n",
      "elettronico.\n",
      "Il biglietto ha validità nella data scelta in fase di acquisto e su di esso riportata.\n",
      "L'ultimo accesso al Museo è consentito 45 minuti prima dell'orario di chiusura.\n",
      "La visita ha la durata di un’ora circa.\n",
      "\n",
      "commissione di servizio non verrà rimborsata. Il presente biglietto deve essere esibito in caso di controllo e conservato per tutta la durata della \n",
      "manifestazione. Non è consentito l’accesso in data, giorno, posto o categoria diversa da quanto previsto sul biglietto. Il possessore del presente \n",
      "titolo, in quanto facente parte del pubblico, acconsente ed autorizza eventuali riprese audio e video che potrebbero essere effettuate durante la\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'The museum does not operate on a fixed \"day of ingress.\" Instead, it offers entrance tickets that can be used on any date purchased during the visit. Each ticket is valid only on its specified purchase date and must be presented at the museum\\'s electronic entry point upon arrival. Visitors are encouraged to check with the museum for their available dates and schedules as the ticket\\'s validity period is tied directly to the chosen date of purchase.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = \"Maranello.pdf\"\n",
    "ask_question(pdf, \"Qual'è il giorno di ingresso al museo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec34c17-9d33-462b-8e2a-96df5ba8544b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
