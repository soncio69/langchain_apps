{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e61027d-96fc-4e97-b977-552a1ec6bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16a5db-c816-4d3d-8c5e-58b7ba710cac",
   "metadata": {},
   "source": [
    "# STRUCTURED OUTPUT\n",
    "\n",
    "Questo notebook presenta alcuni test di utilizzo di structured output con Ollama e LLama3.2\n",
    "\n",
    "Prima di verificare se e come è possibile utilizzare questa funzionalità con Langchain, inizio utilizzando direttamente la libreria di Ollama (che dalla versione rilasciata a dicembre 2024 supporta structured output).\n",
    "\n",
    "Da : https://pavelbazin.com/post/the-essential-guide-to-large-language-models-structured-output-and-function-calling/#how-structured-output-works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd901ab8-651d-4e5e-9ed0-c8144a60b28d",
   "metadata": {},
   "source": [
    "Structured output is a model’s capability to output JSON, acquired during fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a6724-af4a-4ecb-81a2-7fce97086cf0",
   "metadata": {},
   "source": [
    "Come primo step provo a verificare come sia sempre stato possibile produrre output in formato JSON utilizzando le opportune istruzioni da Promt e come tale modalità possa presentare problemi dovuti al fatto che non è possibile \"controllare\" il formato dell'output prodotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57008a38-3068-4396-b560-f5903f2c38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d148cca-1510-46e2-bc1b-e8f552c15f8b",
   "metadata": {},
   "source": [
    "Definisco una funzione che, dati:\n",
    "- un promt\n",
    "- un messaggio di testo fornito dall'utente\n",
    "- un modello (nel nostro caso llama3.2\n",
    "\n",
    "restituisce un output in formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5543842f-76fc-46d1-b5b2-33acf1983922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(prompt: str, message: str, model: str = \"llama3.2:3b-instruct-fp16\"):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "\n",
    "    response = chat(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589f83ff-0d93-46c0-a474-6b1394577bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the grocery list in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"groceries\": [\n",
      "    {\n",
      "      \"item\": \"bread\",\n",
      "      \"quantity\": null\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"eggs\",\n",
      "      \"quantity\": null\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"apples\",\n",
      "      \"quantity\": \"few\"\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"milk\",\n",
      "      \"quantity\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a data parsing assistant. \n",
    "User provides a list of groceries. \n",
    "Your goal is to output it as JSON.\n",
    "\"\"\"\n",
    "message = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n",
    "\n",
    "res = eval(prompt=prompt, message=message)\n",
    "\n",
    "json_data = res['message']['content']\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dcc52-84cd-49ea-80b4-41d6e1d2a3a6",
   "metadata": {},
   "source": [
    "L'output prodotto non è JSON ma è una stringa contenente JSON. POtrebbe essere possibile istruire il modello attraverso il prompt per restituire solo JSON ma non sarebbbe sicuramente una soluzione sicura e affidabile al 100%\n",
    "\n",
    "Questo perchè non è stato utilizzato lo \"structured output\". <br>\n",
    "A tal fine ridefinisco la funzione eval() per utilizzarlo, impostando il formato di output a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d83067-4f44-44a2-96b7-816f42369e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(prompt: str, message: str, model: str = \"llama3.2:3b-instruct-fp16\"):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "\n",
    "    response = chat(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        # Enable structured ouput capability\n",
    "        format=\"json\",\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b8368e-f2e8-4494-92d6-a0e795ac1e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"groceries\": [\"bread\", \"eggs\", \"apples\", \"milk\"]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a data parsing assistant. \n",
    "User provides a list of groceries. \n",
    "Your goal is to output it as JSON.\n",
    "\"\"\"\n",
    "message = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n",
    "\n",
    "res = eval(prompt=prompt, message=message)\n",
    "\n",
    "json_data = res['message']['content']\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0d2b8-3ab4-433a-91a1-1415d059175b",
   "metadata": {},
   "source": [
    "In questo caso abbiamo del JSON valido che è possibile trattare ad esempio per convertirlo in HTML per essere utilizzato da altre funzioni \"standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d085b7-a426-4ff0-b939-dc64f459a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def render(data: str) -> str:\n",
    "    data_dict = json.loads(data)\n",
    "\n",
    "    return f\"\"\"\n",
    "    <ul>\n",
    "        {\"\\n\\t\".join([ f\"<li>{x}</li>\" for x in data_dict[\"groceries\"]])}\n",
    "    </ul>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c806def-7b22-4f7e-9f74-76b56663ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <ul>\n",
      "        <li>bread</li>\n",
      "\t<li>eggs</li>\n",
      "\t<li>apples</li>\n",
      "\t<li>milk</li>\n",
      "    </ul>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(render(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66558360-a448-483a-baf8-3791af3bea7a",
   "metadata": {},
   "source": [
    "In questo caso però il problema è dato dal fatto che non è stato definito uno schema di output per cui il formato JSON di risposta non è stabile tra le diverse invocazioni.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b700d936-2923-4e4e-aaee-8cfffa504e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"groceries\": [\n",
      "    {\"item\": \"eggs\", \"quantity\": 12},\n",
      "    {\"item\": \"milk\", \"quantity\": 2},\n",
      "    {\"item\": \"sparkling water\", \"quantity\": 6}\n",
      "]}\n"
     ]
    }
   ],
   "source": [
    "message = \"12 eggs, 2 bottles of milk, 6 sparkling waters\"\n",
    "res = eval(prompt=prompt, message=message)\n",
    "\n",
    "json_data = res['message']['content']\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93b12d-f6d5-40f7-9277-473a49994994",
   "metadata": {},
   "source": [
    "Per definire un formato di output stabile  è necessario poter definire uno <b>schema</b> di output per il JSON restituito dal modello.\n",
    "\n",
    "Attraverso structured output è possibile anche definire uno schema di output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec47ca80-87d3-402a-a95f-da04ba23b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"groceries\": [\n",
      "    { \n",
      "      \"name\": \"Bread\", \n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    { \n",
      "      \"name\": \"Eggs\", \n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    { \n",
      "      \"name\": \"Apples\", \n",
      "      \"quantity\": 2\n",
      "    },\n",
      "    { \n",
      "      \"name\": \"Milk\", \n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "    \"groceries\": [\n",
      "        {\"name\": \"eggs\", \"quantity\": 12},\n",
      "        {\"name\": \"milk\", \"quantity\": 2},\n",
      "        {\"name\": \"sparkling water\", \"quantity\": 6}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are data parsing assistant. \n",
    "User provides a list of groceries. \n",
    "Use the following JSON schema to generate your response:\n",
    "\n",
    "{{\n",
    "    \"groceries\": [\n",
    "        { \"name\": ITEM_NAME, \"quantity\": ITEM_QUANTITY }\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Name is any string, quantity is a numerical value.\n",
    "\"\"\"\n",
    "\n",
    "inputs = [\n",
    "    \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\",\n",
    "    \"12 eggs, 2 bottles of milk, 6 sparkling waters.\",\n",
    "]\n",
    "\n",
    "for message in inputs:\n",
    "    res = eval(prompt=prompt, message=message)\n",
    "    json_data = res['message']['content']\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc365e-2962-486a-9da0-6b3742495260",
   "metadata": {},
   "source": [
    "Un altro modo per gestire l'ouput di un llm è quello di definire uno schema e definire un metodo per serializzare il JSON in un oggetto definito in modo che possa essere successivamente utilizzato da altri moduli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147ed9f1-bb19-4634-aa2f-0850d9ac82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Any, Self, Generic, Callable, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Define generic type variable\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "# Immutable grocery item container\n",
    "@dataclass(frozen=True)\n",
    "class Item:\n",
    "    name: str\n",
    "    quantity: int\n",
    "\n",
    "\n",
    "# Immutable groceries container\n",
    "@dataclass(frozen=True)\n",
    "class Groceries:\n",
    "    groceries: List[Item]\n",
    "\n",
    "    @staticmethod\n",
    "    def serialize(data: Any) -> Self:\n",
    "        \"\"\"JSON serialization function.\"\"\"\n",
    "        json_data = json.loads(data)\n",
    "        items = [Item(**item) for item in json_data[\"groceries\"]]\n",
    "\n",
    "        return Groceries(groceries=items)\n",
    "\n",
    "\n",
    "# Edited `eval` function to handle types and serialization\n",
    "def eval(prompt: str, \n",
    "         message: str, \n",
    "         schema: Generic[T],\n",
    "         serializer: Callable = None,\n",
    "         model: str = \"llama3.2:3b-instruct-fp16\")-> Optional[T]:\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "\n",
    "    response = chat(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        # Enable structured ouput capability\n",
    "        format=\"json\",\n",
    "    )\n",
    "    try:\n",
    "        data = response['message']['content']\n",
    "        json_data = json.loads(data)\n",
    "\n",
    "        if serializer is not None:\n",
    "            return serializer(data)\n",
    "        else:\n",
    "            return schema(**json_data)\n",
    "    except TypeError as type_error:\n",
    "        # Happens when dictionary data shape doesn't match provided schema layout.\n",
    "        return None\n",
    "    except json.JSONDecodeError as json_error:\n",
    "        # Happens when LLM outputs incorrect JSON, or ``json`` module fails\n",
    "        # to parse it for some other reason.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "178a5802-6718-496c-a5cd-b2ed97680ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groceries(groceries=[Item(name='bread', quantity=1), Item(name='eggs', quantity=1), Item(name='apples', quantity=2), Item(name='milk', quantity=1)])\n"
     ]
    }
   ],
   "source": [
    "res = eval(\n",
    "    prompt=prompt,\n",
    "    message=\"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\",\n",
    "    schema=Groceries,\n",
    "    serializer=Groceries.serialize,\n",
    ")\n",
    "\n",
    "# Pretty print it\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc682fd5-6f5e-4583-9d0d-aed4b8517842",
   "metadata": {},
   "source": [
    "Altra alternativa è quella di utilizzare la funzionalità di structured output resa disponibile da ollama a dicembre 2024.<br>\n",
    "Grazie a questa funzionalità è possibile definire uno schema utilizzando Pydantic e richiamare il modello llama3 passando lo schema creato da Pydantic.\n",
    "\n",
    "TODO : Da approfondire come definire un modello pydantic con un oggetto che comprende una lista di altri oggetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6647e833-067c-45e2-8342-2c229d90e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$defs': {'Item': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'quantity': {'title': 'Quantity', 'type': 'integer'}}, 'required': ['name', 'quantity'], 'title': 'Item', 'type': 'object'}}, 'properties': {'groceries': {'items': {'$ref': '#/$defs/Item'}, 'title': 'Groceries', 'type': 'array'}}, 'required': ['groceries'], 'title': 'Groceries', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Item (BaseModel):\n",
    "    name: str\n",
    "    quantity: int\n",
    "\n",
    "class Groceries(BaseModel):\n",
    "    groceries: List[Item]\n",
    "\n",
    "print(Groceries.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c636569c-95ed-42e7-b034-e66bed51f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groceries=[Item(name='Bread', quantity=1), Item(name='Eggs', quantity=1), Item(name='Apples', quantity=2), Item(name='Milk', quantity=1)]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a data parsing assistant. \n",
    "User provides a list of groceries. \n",
    "Your goal is to output it as JSON.\n",
    "\"\"\"\n",
    "message = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": message},\n",
    "]\n",
    "\n",
    "response = chat(\n",
    "    messages=messages,\n",
    "    model= \"llama3.2:3b-instruct-fp16\",\n",
    "    # Enable structured ouput capability\n",
    "    format=Groceries.model_json_schema(),\n",
    ")\n",
    "\n",
    "groceries = Groceries.model_validate_json(response.message.content)\n",
    "print(groceries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c42d7e-28eb-4255-a7c5-1387bf25a556",
   "metadata": {},
   "source": [
    "Function calling is a type of structured output capability of a large language model.”\n",
    "\n",
    "LLMs don’t call any functions themselves; they suggest which function you should call from pre-defined functions which you provide to the LLM in a prompt.\n",
    "\n",
    "Therefore, function calling is nothing but structured output, or a special case of structured output. \n",
    "\n",
    "Function calling capability is achieved via fine-tuning of a model\n",
    "\n",
    "it is just JSON formatted output which contains the name of a function to call and parameters for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03bbb51f-90d2-491e-8d91-09854551747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is three plus one?\n",
      "Calling function: add_two_numbers\n",
      "Arguments: {'a': 3, 'b': 1}\n",
      "Function output: 4\n",
      "Final response: The answer to the equation 3 + 1 is indeed 4.\n"
     ]
    }
   ],
   "source": [
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a (int): The first number\n",
    "    b (int): The second number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "  return a + b\n",
    "\n",
    "\n",
    "def subtract_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Subtract two numbers\n",
    "  \"\"\"\n",
    "  return a - b\n",
    "\n",
    "messages = [{'role': 'user',\n",
    "             'content': 'What is three plus one?'}]\n",
    "\n",
    "print('Prompt:', messages[0]['content'])\n",
    "\n",
    "available_functions = {\n",
    "  'add_two_numbers': add_two_numbers,\n",
    "  'subtract_two_numbers': subtract_two_numbers,\n",
    "}\n",
    "\n",
    "response: ChatResponse = chat(\n",
    "  model= \"llama3.2:3b-instruct-fp16\",\n",
    "  messages=messages,\n",
    "  tools=[add_two_numbers, subtract_two_numbers],\n",
    ")\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  \n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool in response.message.tool_calls:\n",
    "    \n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool.function.name):\n",
    "      print('Calling function:', tool.function.name)\n",
    "      print('Arguments:', tool.function.arguments)\n",
    "      output = function_to_call(**tool.function.arguments)\n",
    "      print('Function output:', output)\n",
    "    else:\n",
    "      print('Function', tool.function.name, 'not found')\n",
    "\n",
    "# Only needed to chat with the model using the tool call results\n",
    "if response.message.tool_calls:\n",
    "\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "  messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})\n",
    "\n",
    "  # Get final response from model with function outputs\n",
    "  final_response = chat(\n",
    "      model= \"llama3.2:3b-instruct-fp16\",\n",
    "      messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360d503-beff-4aee-8171-241314b0432c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
